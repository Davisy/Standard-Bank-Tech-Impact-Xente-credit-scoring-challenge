{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Bank Tech Impact Challenge: Xente credit scoring challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to read the Zindi Competition details in the zindi platform, [Zindi Africa](https://zindi.africa/competitions/sbtic-xente-credit-scoring-challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import important modules \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from boruta import BorutaPy  \n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import chi2 \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading the files and loading them into dataframes.\n",
    "train = pd.read_csv('../data/Train.csv')\n",
    "test= pd.read_csv('../data/Test.csv')\n",
    "#sample = pd.read_csv('../../data/sample_submission.csv')\n",
    "mask = pd.read_csv('../data/unlinked_masked_final.csv')\n",
    "variabs = pd.read_csv('../data/VariableDefinitions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomerId                 0\n",
      "TransactionStartTime       0\n",
      "Value                      0\n",
      "Amount                     0\n",
      "TransactionId              0\n",
      "BatchId                    0\n",
      "SubscriptionId             0\n",
      "CurrencyCode               0\n",
      "CountryCode                0\n",
      "ProviderId                 0\n",
      "ProductId                  0\n",
      "ProductCategory            0\n",
      "ChannelId                  0\n",
      "TransactionStatus          0\n",
      "IssuedDateLoan           612\n",
      "AmountLoan               612\n",
      "Currency                 612\n",
      "LoanId                   612\n",
      "PaidOnDate               612\n",
      "IsFinalPayBack           612\n",
      "InvestorId               612\n",
      "DueDate                  614\n",
      "LoanApplicationId        617\n",
      "PayBackId                612\n",
      "ThirdPartyId             614\n",
      "IsThirdPartyConfirmed    612\n",
      "IsDefaulted              612\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check missing values \n",
    "print(train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXPLORATORY DATA ANALYSIS \n",
    "\n",
    "#import the package \n",
    "import pandas_profiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate report \n",
    "eda_report = pandas_profiling.ProfileReport(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export into html fie \n",
    "eda_report.to_file(\"eda_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Specs         Score\n",
      "35            before_due_min  1.036756e+08\n",
      "1                      Value  7.545044e+07\n",
      "33           before_due_mean  7.270749e+07\n",
      "36            before_due_max  4.363295e+07\n",
      "29           max_cus_transac  1.241756e+06\n",
      "27           std_cus_transac  2.044060e+05\n",
      "28           min_cus_transac  5.541087e+04\n",
      "34            before_due_std  3.371891e+04\n",
      "26          mean_cus_transac  1.988354e+04\n",
      "15              ProductId_14  5.532635e+02\n",
      "22         ProductCategory_6  5.532635e+02\n",
      "32            inc_value_date  2.737818e+02\n",
      "24  Number_Of_Split_Payments  8.675770e+01\n",
      "25      Count_Rejected_Loans  8.099436e+01\n",
      "37        Cnt_missed_payment  2.716352e+01\n",
      "17         ProductCategory_1  2.102796e+01\n",
      "30               Day_Of_Week  1.003593e+01\n",
      "5                ProductId_4  9.948098e+00\n",
      "2                ProductId_1  9.885831e+00\n",
      "18         ProductCategory_2  9.190848e+00\n",
      "0                 CustomerId  7.415284e+00\n",
      "3                ProductId_2  4.348468e+00\n",
      "6                ProductId_5  4.199630e+00\n",
      "11              ProductId_10  4.199630e+00\n",
      "31              Day_in_month  4.043477e+00\n"
     ]
    }
   ],
   "source": [
    "## FEATURE ENGINEERING \n",
    "\n",
    "# import category encoder \n",
    "import category_encoders as ce\n",
    "\n",
    "## Transform dates types from 'object' to 'datetime'\n",
    "train.TransactionStartTime=pd.to_datetime(train.TransactionStartTime)\n",
    "test.TransactionStartTime=pd.to_datetime(test.TransactionStartTime)\n",
    "train.IssuedDateLoan=pd.to_datetime(train.IssuedDateLoan)\n",
    "test.IssuedDateLoan=pd.to_datetime(test.IssuedDateLoan)\n",
    "train.PaidOnDate=pd.to_datetime(train.PaidOnDate)\n",
    "train.DueDate=pd.to_datetime(train.DueDate)\n",
    "\n",
    "## creating variables to transfer the information contained in the rows of the same transaction.\n",
    "train['Number_Of_Split_Payments'] = 0 ## this is a count on the number of payments on the same loan. It will take a 0 for singled-rowed transactions, 1+ for multi-row transacs.\n",
    "#train['Sum_Diff_Time_Payments'] = 0 ## I'm thinking of summing the delays between all payments made on a loan. It will take 0 for loans paid in a single time, 1+ for multiple payments on the same loan.\n",
    "test['Number_Of_Split_Payments']=0\n",
    "#test['Sum_Diff_Time_Payments']=0\n",
    "\n",
    "## creating the feature : number of split payments on a loan.\n",
    "train['Number_Of_Split_Payments']=train['TransactionId'].map(train.groupby('TransactionId').agg('count')['Number_Of_Split_Payments'])\n",
    "test['Number_Of_Split_Payments']=test['TransactionId'].map(test.groupby('TransactionId').agg('count')['Number_Of_Split_Payments'])\n",
    "\n",
    "train.drop(train[(train.TransactionId=='TransactionId_703')|((train.TransactionId=='TransactionId_927'))].index,axis=0,inplace=True)\n",
    "\n",
    "## Lets drop the duplicate rows with the same transaction ID and keep the last one. (as in with the latest payment installment )\n",
    "train.drop_duplicates(subset=['TransactionId'],keep='last',inplace=True)\n",
    "test.drop_duplicates(subset=['TransactionId'],keep='last',inplace=True)\n",
    "\n",
    "train.drop(['CountryCode','Currency','CurrencyCode','SubscriptionId','ProviderId','ChannelId'],axis=1,inplace=True)\n",
    "test.drop(['CountryCode','CurrencyCode','SubscriptionId','ProviderId','ChannelId'],axis=1,inplace=True)\n",
    "\n",
    "# Feature Engineering \n",
    "train['Count_Rejected_Loans'] = train['CustomerId'].map(train[train.TransactionStatus==0].groupby('CustomerId').LoanId.size())\n",
    "test['Count_Rejected_Loans'] = test['CustomerId'].map(train[train.TransactionStatus==0].groupby('CustomerId').LoanId.size())\n",
    "## then we should impute the columns of customers that were not found in the rejected list with 0 as in they have never been rejected.\n",
    "train.Count_Rejected_Loans.fillna(value=0,inplace=True)\n",
    "test.Count_Rejected_Loans.fillna(value=0,inplace=True)\n",
    "\n",
    "## group train/test together to perform cumulative count\n",
    "all_data=pd.concat((train,test))\n",
    "## Initialize and compute values for the new feature\n",
    "all_data['Cumulative_Reject']=0\n",
    "all_data.loc[all_data.TransactionStatus==0,'Cumulative_Reject'] = all_data[all_data.TransactionStatus==0].groupby('CustomerId').cumcount()\n",
    "## Separate all_data into train and test\n",
    "train1=all_data[:len(train)]\n",
    "test1=all_data[len(train):]\n",
    "train['Cumulative_Reject']=0\n",
    "test['Cumulative_Reject']=0\n",
    "train['Cumulative_Reject']=train1['Cumulative_Reject']\n",
    "test['Cumulative_Reject']=test1['Cumulative_Reject']\n",
    "\n",
    "purchasestats=train[train.TransactionStatus==0].groupby('CustomerId').Value.agg(('mean','std','min','max'))\n",
    "train['prchs_mean']=train['CustomerId'].map(purchasestats['mean'])\n",
    "train['prchs_std']=train['CustomerId'].map(purchasestats['std'])\n",
    "train['prchs_max']=train['CustomerId'].map(purchasestats['max'])\n",
    "train['prchs_min']=train['CustomerId'].map(purchasestats['min'])\n",
    "test['prchs_mean']=test['CustomerId'].map(purchasestats['mean'])\n",
    "test['prchs_std']=test['CustomerId'].map(purchasestats['std'])\n",
    "test['prchs_max']=test['CustomerId'].map(purchasestats['max'])\n",
    "test['prchs_min']=test['CustomerId'].map(purchasestats['min'])\n",
    "\n",
    "valuegroups=mask.groupby('CustomerId').Value.agg(('mean','std','min','max'))\n",
    "train['mean_cus_transac']=train['CustomerId'].map(valuegroups['mean'])\n",
    "train['std_cus_transac']=train['CustomerId'].map(valuegroups['std'])\n",
    "train['min_cus_transac']=train['CustomerId'].map(valuegroups['min'])\n",
    "train['max_cus_transac']=train['CustomerId'].map(valuegroups['max'])\n",
    "test['mean_cus_transac']=test['CustomerId'].map(valuegroups['mean'])\n",
    "test['std_cus_transac']=test['CustomerId'].map(valuegroups['std'])\n",
    "test['min_cus_transac']=test['CustomerId'].map(valuegroups['min'])\n",
    "test['max_cus_transac']=test['CustomerId'].map(valuegroups['max'])\n",
    "\n",
    "train['Day_Of_Week']= train.TransactionStartTime.dt.weekday\n",
    "test['Day_Of_Week'] =test.TransactionStartTime.dt.weekday\n",
    "train['Day_in_month']=train.TransactionStartTime.dt.day\n",
    "test['Day_in_month']=test.TransactionStartTime.dt.day\n",
    "\n",
    "from datetime import date\n",
    "datemin = date(2018,9,21)\n",
    "datemax= date(2019,7,17)\n",
    "(datemax-datemin).days\n",
    "datesinc=pd.DataFrame(columns=['date','inc_value'])\n",
    "datesinc.loc[0,'inc_value']=1\n",
    "datesinc.loc[0,'date']=datemin\n",
    "from datetime import timedelta\n",
    "for i in range(2,301):\n",
    "    datesinc.loc[i-1,'inc_value']=i\n",
    "    datesinc.loc[i-1,'date']=datemin + timedelta(days=i-1)\n",
    "train['inc_value_date']=train.TransactionStartTime.dt.date.map(datesinc.set_index('date').inc_value)\n",
    "test['inc_value_date']=test.TransactionStartTime.dt.date.map(datesinc.set_index('date').inc_value)\n",
    "\n",
    "train.inc_value_date = train.inc_value_date.astype(np.int64)\n",
    "test.inc_value_date = test.inc_value_date.astype(np.int64)\n",
    "\n",
    "aa=train[(train.TransactionStatus==1)&(train.TransactionStartTime<train.DueDate)].groupby('CustomerId').agg(('count','mean','std','min','max')).Value\n",
    "#train['number_transac_before_due']=train['CustomerId'].map(aa['count'])\n",
    "train['before_due_mean'] = train['CustomerId'].map(aa['mean'])\n",
    "train['before_due_std'] = train['CustomerId'].map(aa['std'])\n",
    "train['before_due_min'] = train['CustomerId'].map(aa['min'])\n",
    "train['before_due_max'] = train['CustomerId'].map(aa['max'])\n",
    "test['before_due_mean'] = test['CustomerId'].map(aa['mean'])\n",
    "test['before_due_std'] = test['CustomerId'].map(aa['std'])\n",
    "test['before_due_min'] = test['CustomerId'].map(aa['min'])\n",
    "test['before_due_max'] = test['CustomerId'].map(aa['max'])\n",
    "\n",
    "\n",
    "train['Cnt_missed_payment']=0\n",
    "train.loc[train.DueDate<train.PaidOnDate,'Cnt_missed_payment']=train[train.DueDate<train.PaidOnDate].groupby('CustomerId').cumcount()\n",
    "test['Cnt_missed_payment']=test['CustomerId'].map(train.groupby('CustomerId').agg('max').Cnt_missed_payment)\n",
    "\n",
    "train=train[train.IsDefaulted.notnull()]\n",
    "\n",
    "features = ['CustomerId', #'TransactionStartTime', \n",
    "            'Value', #'Amount',\n",
    "       #'TransactionId', #'BatchId', \n",
    "             'ProductId',\n",
    "       'ProductCategory', #'TransactionStatus', \n",
    "            #'IssuedDateLoan',\n",
    "       #'LoanId', 'InvestorId', 'LoanApplicationId', 'ThirdPartyId',\n",
    "       'Number_Of_Split_Payments', 'Count_Rejected_Loans', \n",
    "           #'Cumulative_Reject',\n",
    "       #'prchs_mean', 'prchs_std', 'prchs_max', 'prchs_min', \n",
    "            'mean_cus_transac','std_cus_transac', 'min_cus_transac', 'max_cus_transac', \n",
    "            'Day_Of_Week','Day_in_month', 'inc_value_date', 'before_due_mean', 'before_due_std',\n",
    "       'before_due_min', 'before_due_max', \n",
    "            'Cnt_missed_payment'\n",
    "]\n",
    "\n",
    "\n",
    "oce = ce.OneHotEncoder(cols=['ProductId','ProductCategory'])\n",
    "tce = ce.TargetEncoder(cols=['CustomerId'],smoothing=40,min_samples_leaf=3)\n",
    "\n",
    "X = train[features]\n",
    "\n",
    "#handle missing values in training set\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "X_test = test[features]\n",
    "#handle missing values in training set\n",
    "X_test = X_test.fillna(X_test.median())\n",
    "\n",
    "y=train.IsDefaulted.copy()\n",
    "X = oce.fit_transform(X)\n",
    "X = tce.fit_transform(X,y)\n",
    "X_test = oce.transform(X_test)\n",
    "X_test = tce.transform(X_test)\n",
    "\n",
    "\n",
    "#FEATURE SELECTION\n",
    "\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=25)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(25,'Score'))  #print 10 best features\n",
    "\n",
    "best_features_df = featureScores.nlargest(25,'Score')\n",
    "\n",
    "best_features_cols = list(best_features_df.Specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "#select best features \n",
    "\n",
    "X = X[best_features_cols]\n",
    "X_test = X_test[best_features_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and valid \n",
    "X_Train, X_val, y_Train, y_val = train_test_split(X,y, test_size=0.05,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEW CLASIFIERS FROM SCIKIT LEARN\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_hist_gradient_boosting \n",
    "\n",
    "# now you can import normally from ensemble \n",
    "from sklearn.ensemble import HistGradientBoostingClassifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer = HistGradientBoostingClassifier() \n",
    "\n",
    "parameters = {\n",
    "    'max_iter': [100,200,500,700,1000,1200,1500],\n",
    "    'learning_rate': [0.1,0.05,1],\n",
    "    'max_depth' : [25, 50, 75],\n",
    "    'l2_regularization': [1.5],\n",
    "    'scoring': ['f1_micro','balanced_accuracy','roc_auc'],\n",
    "    'random_state' : [42],\n",
    "    'verbose':[2],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "pipeline_optimizer_grid = GridSearchCV(pipeline_optimizer, parameters, n_jobs=2, \n",
    "                   cv=5, scoring='roc_auc',\n",
    "                   verbose=2, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle imbalance of data in the trainset \n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "sm = SMOTEENN(random_state=42)\n",
    "X_train, y_train = sm.fit_resample(X_Train, y_Train)\n",
    "\n",
    "# convert into Dataframe\n",
    "features = list(X.columns)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               learning_rate=0.1, max_depth=-1, min_child_samples=20,\n",
       "               min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
       "               n_jobs=-1, num_leaves=31, objective=None, random_state=None,\n",
       "               reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "               subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training \n",
    "pipeline_optimizer_grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9482758620689655\n"
     ]
    }
   ],
   "source": [
    "#find the performance\n",
    "print(pipeline_optimizer_grid.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted result for the test Data\n",
    "preds = pipeline_optimizer_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.DataFrame(columns=['TransactionId','IsDefaulted'])\n",
    "sample_submission['TransactionId'] = test['TransactionId']\n",
    "sample_submission['IsDefaulted'] = preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission csv file\n",
    "from datetime import datetime\n",
    "now = datetime. now()\n",
    "current_time = now.strftime(\"%S\")\n",
    "\n",
    "sample_submission.to_csv('../data/submissions/my_submission_{}_sun.csv'.format(current_time), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
